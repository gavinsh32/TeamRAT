# -*- coding: utf-8 -*-
"""rat-sam-tuning

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1YIq4hBZXs0xiC6_rNYoaZRoQeSzkDrJz

# Fine-Tune SAM

Gavin Haynes

Fine-tune SAM to segment fascicles from micro-CT scans of rat tails.

Thanks to Alex Bonnet for [this awesome example](https://colab.research.google.com/github/encord-team/encord-notebooks/blob/main/colab-notebooks/Encord_Notebooks_How_To_Fine_Tuning_SAM.ipynb#scrollTo=YkwevXNd2Ofw).

# Dataset Setup

Import the dataset, extract necessary information, squash masks

## Install Prereqs and and Fetch Data
"""

import json
import torch
from segment_anything.utils.transforms import ResizeLongestSide
from segment_anything import sam_model_registry
import os
import numpy as np
import cv2 as cv
import matplotlib.pyplot as plt
from pycocotools import mask as pycmask
from pathlib import Path
from collections import defaultdict

"""Read in JSON and extract necessary data"""

data_path = Path('labels.json')
imgs_folder = Path('imgs')

data = []
with open(data_path, 'r') as f:
    data = json.load(f)

combined_ground_truth_masks = {}

# Convert RLE counts to binary from utf-8 encoding
for entry in data:

    info = entry['image']
    height = info['height']
    width = info['width']
    k = info['file_name']
    mask = np.zeros((height, width)).astype(np.uint8)

    for ann in entry['annotations']:
        seg = ann['segmentation']
        counts = seg['counts']
        seg['counts'] = bytes(counts, 'utf-8')
        decoded_mask = pycmask.decode(seg)
        decoded_mask = decoded_mask.astype(np.uint8)
        mask |= decoded_mask

    mask = mask.astype(np.float32)

    combined_ground_truth_masks[k] = mask

"""Display Sample Masks"""

print(data)

"""Helper functions for displaying masks"""

def draw_mask_grid(img, grid: list):
    """
    Draw points from a grid on an image.
    Args:
        mask (np.ndarray.float32): Binary mask.
        grid (list[list]): List of point coordinates in format [y, x].
    Returns:
        np.ndarray.float32: Image with points drawn.
    """

    for point in grid:

        loc = (point[0], point[1])
        color = (255, 0, 0)
        cv.circle(img, loc, 3, color, -1)

    return img

def make_point_grid(height: int, width: int, n: int, padding: int):
    """
    Return a point prompt grid of n x n points.
    Args:
        height (int): Height of the image.
        width (int): Width of the image.
        n (int): Number of points per side.
        padding (int): Padding around the grid.
    Returns:
        list: List of numpy points [y, x].
    """

    start_y, start_x = padding, padding

    end_y, end_x = height - padding, width - padding

    x_step = (end_x - start_x) // n
    y_step = (end_y - start_y) // n

    x_points = [x for x in range(start_x, end_x + 1, x_step)]
    y_points = [y for y in range(start_y, end_y + 1, y_step)]

    points = []
    for y in y_points:
        for x in x_points:
            points.append([y, x])

    return points

def draw_mask(img: cv.typing.MatLike, mask: cv.typing.MatLike):
    """
    Draw a mask on an image.
    Args:
        mask (np.ndarray.float32): Binary mask.
    Returns:
        np.ndarray.float32: Image with mask drawn.
    """

    mask = mask.astype(np.uint8)
    img = cv.addWeighted(img, 1, mask, 0.3, 0)

    return img

"""### SAM Preprocessing

Fetch Model
"""

model_type = 'vit_b'
checkpoint = 'sam_vit_b_01ec64.pth'
device = 'cuda:0'

from segment_anything import SamPredictor, sam_model_registry

sam_model = sam_model_registry[model_type](checkpoint=checkpoint)
sam_model.to(device)
sam_model.train()

"""Convert input data to tensors"""

transformed_data = defaultdict(dict)
transform = ResizeLongestSide(sam_model.image_encoder.img_size)

for entry in data:

    k = entry['image']['file_name']

    img = cv.imread(imgs_folder / k)
    img = cv.cvtColor(img, cv.COLOR_BGR2RGB)

    input_img = transform.apply_image(img)

    input_img = torch.as_tensor(input_img, device=device)
    input_img = input_img.permute(2, 0, 1).contiguous()[None, :, :, :]

    input_img = sam_model.preprocess(input_img)
    input_size = tuple(input_img.shape[-2:])
    original_image_size = img.shape[:2]

    transformed_data[k]['image'] = input_img
    transformed_data[k]['input_size'] = input_size
    transformed_data[k]['original_image_size'] = original_image_size

"""# Training

Setup Optimizer
"""

# Set up the optimizer, hyperparameter tuning will improve performance here
lr = 1e-4
wd = 0
optimizer = torch.optim.Adam(sam_model.mask_decoder.parameters(), lr=lr, weight_decay=wd)
loss_fn = torch.nn.BCELoss()
keys = list(transformed_data.keys())

"""Training Loop"""

# --- MODIFIED SECTION START ---

import torch # Ensure torch is imported
import torch.nn.functional as F # For resizing GT mask if needed
from statistics import mean
from tqdm import tqdm # For progress bar

# --- Setup Optimizer and Loss ---
lr = 1e-5 # Adjusted learning rate - tune as needed
wd = 0
optimizer = torch.optim.Adam(sam_model.mask_decoder.parameters(), lr=lr, weight_decay=wd)

# Use BCEWithLogitsLoss - more stable and takes raw model outputs (logits)
loss_fn = torch.nn.BCEWithLogitsLoss()

# --- Prepare Keys and Point Grid ---
# Ensure keys are consistent between ground truth and preprocessed images
keys = list(k for k in combined_ground_truth_masks.keys() if k in transformed_data)

num_points = 19
padding = 20

full_grid_points = np.array(
    make_point_grid(660, 660, num_points, padding)
)

# --- Training Loop ---
num_epochs = 30 # Or your desired number
losses = []

for epoch in range(num_epochs):
    sam_model.train() # Ensure model is in training mode each epoch
    epoch_losses = []

    # Use tqdm for a progress bar over the keys
    for k in tqdm(keys[:30], desc=f"Epoch {epoch+1}/{num_epochs}"):
        # --- Get Preprocessed Data ---
        input_image = transformed_data[k]['image'].to(device)
        input_size = transformed_data[k]['input_size'] # Size SAM processed the image at
        original_image_size = transformed_data[k]['original_image_size'] # Original H, W

        # --- Generate Embeddings (No gradient needed for encoders) ---
        with torch.no_grad():
            image_embedding = sam_model.image_encoder(input_image)

            # --- Prepare Point Grid Prompt ---
            # Apply coordinate transformation from original image space to SAM's input space
            points_coords_transformed = transform.apply_coords(full_grid_points, original_image_size)
            points_torch = torch.as_tensor(points_coords_transformed, dtype=torch.float, device=device)

            # Add batch dimension: (1, N, 2)
            points_torch = points_torch.unsqueeze(0)
            # Create labels tensor (1 for foreground point prompt): (1, N)
            labels_torch = torch.ones(points_torch.shape[1], dtype=torch.int, device=device).unsqueeze(0)

            # --- Call Prompt Encoder with Points ---
            sparse_embeddings, dense_embeddings = sam_model.prompt_encoder(
                points=(points_torch, labels_torch), # Pass points tuple
                boxes=None,                         # No boxes
                masks=None,
            )
            # ------------------------------------

        # --- Mask Decoder ---
        # No multimask output, get single best prediction relative to prompt
        low_res_masks, iou_predictions = sam_model.mask_decoder(
            image_embeddings=image_embedding,
            image_pe=sam_model.prompt_encoder.get_dense_pe(),
            sparse_prompt_embeddings=sparse_embeddings,
            dense_prompt_embeddings=dense_embeddings,
            multimask_output=False,
        )

        # --- Upscale Mask & Prepare for Loss ---
        # Upscale to original image size - output contains raw logits
        upscaled_masks = sam_model.postprocess_masks(
            low_res_masks,
            input_size=input_size,
            original_size=original_image_size
        ).to(device) # Shape: (1, 1, H, W)

        # --- Prepare Ground Truth Mask for Loss ---
        # Fetch the pre-combined mask (should be float32, 0.0 or 1.0)
        gt_mask = combined_ground_truth_masks[k]

        # Convert to tensor, add batch and channel dims: (1, 1, H_orig, W_orig)
        gt_mask_torch = torch.from_numpy(gt_mask).unsqueeze(0).unsqueeze(0).to(device)

        # Ensure GT mask spatial dimensions match model output (upscaled_masks)
        target_height, target_width = upscaled_masks.shape[-2:]
        if gt_mask_torch.shape[-2:] != (target_height, target_width):
            # Resize GT mask to match predicted mask size using nearest neighbor interpolation
             print(f"Warning: Resizing GT mask for {k} from {gt_mask_torch.shape[-2:]} to {(target_height, target_width)}")
             gt_mask_torch = F.interpolate(gt_mask_torch,
                                           size=(target_height, target_width),
                                           mode='nearest')

        # --- Calculate Loss ---
        # BCEWithLogitsLoss takes raw logits (upscaled_masks) and float GT mask
        loss = loss_fn(upscaled_masks, gt_mask_torch)

        # --- Optimizer Step ---
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
        epoch_losses.append(loss.item())
        # --- End of loop for one image ---

    # --- End of Epoch ---
    losses.append(epoch_losses) # Store losses for the epoch
    # Calculate and print mean loss for the epoch
    mean_epoch_loss = mean(epoch_losses) if epoch_losses else 0
    print(f'EPOCH: {epoch+1}/{num_epochs}')
    print(f'Mean loss: {mean_epoch_loss:.6f}')

# --- Training Finished ---
print("Training complete.")

mean_losses = [mean(x) if x else 0 for x in losses] # Calculate mean loss per epoch
plt.plot(range(1, num_epochs + 1), mean_losses)
plt.title('Mean Epoch Loss')
plt.xlabel('Epoch Number')
plt.ylabel('Loss')
plt.show()

"""Compare original and trained models."""

import matplotlib.pyplot as plt
from segment_anything import SamPredictor

# Load the trained SAM model
sam_model_trained = sam_model  # Assuming 'sam_model' is your trained model

# Load the original, untrained SAM model
sam_model_untrained = sam_model_registry[model_type](checkpoint=checkpoint)
sam_model_untrained.to(device)

# Create predictors for both models
predictor_trained = SamPredictor(sam_model_trained)
predictor_untrained = SamPredictor(sam_model_untrained)

# Choose an image for inference
image_path = 'imgs/105.jpg'  # Replace with the path to your image
image = cv.imread(image_path)
image = cv.cvtColor(image, cv.COLOR_BGR2RGB)

# Perform inference with both models
predictor_trained.set_image(image)
predictor_untrained.set_image(image)

# Generate masks using a point prompt (example)
# Generate masks using a point prompt (example)
input_points = np.array(full_grid_points)
input_labels = np.ones(input_points.shape[0], dtype=np.int32)  # All points are foreground

masks_trained, _, _ = predictor_trained.predict(
    point_coords=input_points,
    point_labels=input_labels,
    multimask_output=False,
)
masks_untrained, _, _ = predictor_untrained.predict(
    point_coords=input_points,
    point_labels=input_labels,
    multimask_output=False,
)

# Visualize the results
plt.figure(figsize=(10, 5))

plt.subplot(1, 2, 1)
plt.imshow(image)
plt.imshow(masks_trained[0], alpha=0.5)
plt.title('Trained Model')
plt.savefig('trained_model.png')

plt.subplot(1, 2, 2)
plt.imshow(image)
plt.imshow(masks_untrained[0], alpha=0.5)
plt.title('Untrained Model')
plt.savefig('untrained_model.png')

plt.show()

torch.save(sam_model.state_dict(), 'trained-temp.pth')